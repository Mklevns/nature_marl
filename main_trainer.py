# File: nature_marl/main_trainer.py
#!/usr/bin/env python3
"""
Unified Orchestrator for Nature-Inspired Multi-Agent Reinforcement Learning

This script serves as the main entry point for training and experimentation, merging
functionalities for hardware optimization, environment setup, model configuration,
and training execution into a single, robust workflow.

It is built upon the Ray RLlib 2.9.x API stack (RLModule, AlgorithmConfig).

Example Usage:
  # Run a basic foraging experiment on CPU for 50 iterations
  python main_trainer.py --env foraging --agents 5 --iterations 50 --hardware cpu

  # Run a more complex predator-prey experiment on GPU
  python main_trainer.py --env predator_prey --agents 10 --iterations 200 --hardware gpu

  # Run system-level tests without training
  python main_trainer.py --test-only
"""

import sys
import argparse
import warnings
import traceback
from pathlib import Path
from datetime import datetime
import time
import json
import numpy as np

# Suppress various warnings for cleaner output
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=UserWarning, module="pygame")

# Add current directory to path for local imports
sys.path.append(str(Path(__file__).parent))

# --- Core MARL-DevGPT Modules ---
from emergence_environments import create_emergence_environment
from reward_engineering import RewardEngineer, RewardPresets
from reward_engineering_wrapper import RewardEngineeringWrapper
from communication_metrics import CommunicationAnalyzer
from real_emergence_callbacks import RealEmergenceTrackingCallbacks
from rl_module import NatureInspiredCommModule
from training_config import TrainingConfigFactory, print_hardware_summary

# --- Ray RLlib 2.9.0 Core Imports ---
import ray
from ray import tune
from ray.rllib.algorithms.ppo import PPOConfig
from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv
from ray.rllib.core.rl_module.rl_module import SingleAgentRLModuleSpec
from ray.tune.registry import register_env
from ray.tune import Tuner, TuneConfig, TuneError


def create_env_with_wrappers(env_config_from_ray: dict):
    """
    Global environment creator function for Ray Tune.
    Instantiates the base environment and applies necessary wrappers.
    """
    # Extract parameters for the base environment
    env_type = env_config_from_ray.get("env_type", "foraging")
    n_agents = env_config_from_ray.get("n_agents", 5)

    # Create the base biologically-inspired environment from emergence_environments.py
    base_env = create_emergence_environment(
        env_type=env_type,
        n_agents=n_agents,
        grid_size=env_config_from_ray.get("grid_size", (30, 30)),
        episode_length=env_config_from_ray.get("episode_length", 200),
        communication_dim=env_config_from_ray.get("comm_dim", 8),
        render_mode=env_config_from_ray.get("render_mode", None)
    )

    # Wrap for PettingZoo compatibility
    pz_env = ParallelPettingZooEnv(base_env)

    # Apply the RewardEngineeringWrapper to inject communication events and shape rewards
    # We select a reward preset based on the environment type for demonstration
    if env_type == "foraging":
        reward_engineer_instance = RewardPresets.ant_colony_foraging()['engineer']
    elif env_type == "predator_prey":
        reward_engineer_instance = RewardPresets.predator_prey_defense()['engineer']
    else:
        # Default reward engineer
        reward_engineer_instance = RewardEngineer(env_type)
        reward_engineer_instance.create_implicit_reward_structure("multi_principle")

    communication_analyzer_instance = CommunicationAnalyzer()

    wrapped_env = RewardEngineeringWrapper(
        pz_env,
        reward_engineer=reward_engineer_instance,
        communication_analyzer=communication_analyzer_instance
    )
    return wrapped_env


class UnifiedMARLTrainer:
    """
    Orchestrates the entire training pipeline from configuration to execution.
    """
    def __init__(self, args: argparse.Namespace):
        self.args = args
        self.training_config_factory = TrainingConfigFactory()
        self.hardware_info = self.training_config_factory.hardware_info

        # Setup output directories
        self.results_base_dir = Path(args.output_dir)
        self.results_base_dir.mkdir(parents=True, exist_ok=True)
        self.exp_run_dir = self.results_base_dir / f"{args.env}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.exp_run_dir.mkdir(exist_ok=True)
        print(f"üìÅ Experiment results will be saved to: {self.exp_run_dir}")

    def _get_ppo_config(self) -> PPOConfig:
        """
        Creates a hardware-optimized PPOConfig for the specified experiment.
        """
        # Create a dummy environment to inspect observation and action spaces
        dummy_env = create_env_with_wrappers(self.args.env_config)
        single_agent_obs_space = dummy_env.observation_space["agent_0"]
        single_agent_action_space = dummy_env.action_space["agent_0"]
        dummy_env.close()

        # Get the optimal configuration from the factory based on detected hardware
        config = self.training_config_factory.create_config(
            single_agent_obs_space,
            single_agent_action_space,
            force_mode=self.args.hardware
        )

        # Specify the RLModule with its custom configuration
        rl_module_spec = SingleAgentRLModuleSpec(
            module_class=NatureInspiredCommModule,
            observation_space=single_agent_obs_space,
            action_space=single_agent_action_space,
            model_config_dict={
                "num_agents": self.args.agents,
                "embed_dim": 256,
                "pheromone_dim": self.args.comm_dim,
                "memory_dim": self.args.memory_dim,
                "num_comm_rounds": self.args.comm_rounds,
                "num_attention_heads": self.args.attention_heads,
            }
        )
        config.rl_module(rl_module_spec=rl_module_spec)

        # Set the environment creator function for Ray workers
        config.environment(env="unified_marl_env", env_config=self.args.env_config)

        # Configure multi-agent policies for parameter sharing
        config.multi_agent(
            policies={"shared_policy": (None, single_agent_obs_space, single_agent_action_space, {})},
            policy_mapping_fn=lambda agent_id, episode, **kwargs: "shared_policy",
        )

        # Add our custom callbacks for tracking emergent communication
        config.callbacks(RealEmergenceTrackingCallbacks)

        return config

    def run_training(self):
        """
        Initializes Ray, builds the algorithm, and executes the training loop.
        """
        print_hardware_summary()

        # Initialize Ray
        if ray.is_initialized():
            ray.shutdown()

        # NOTE: NVMe usage would be configured here if get_ray_init_kwargs supported it.
        # For simplicity, we manage CPU/GPU resources.
        ray.init(
            num_gpus=1 if self.hardware_info.gpu_available and self.args.hardware == 'gpu' else 0,
            num_cpus=self.hardware_info.cpu_cores_logical,
            include_dashboard=False # Can be set to True for debugging
        )
        print(f"‚úÖ Ray {ray.__version__} initialized successfully!")
        print(f"üìä Ray Cluster Resources: {ray.cluster_resources()}")

        # Register the environment creator with Ray Tune
        register_env("unified_marl_env", create_env_with_wrappers)

        # Get the full PPO configuration
        ppo_config = self._get_ppo_config()

        # Setup and run the Tuner
        tuner = tune.Tuner(
            "PPO",
            param_space=ppo_config.to_dict(),
            run_config=RunConfig(
                name=f"EmergenceExp_{self.args.env}",
                stop={"training_iteration": self.args.iterations},
                checkpoint_config=tune.CheckpointConfig(
                    checkpoint_frequency=self.args.checkpoint_freq,
                    num_to_keep=3,
                    checkpoint_at_end=True,
                ),
                failure_config=FailureConfig(max_failures=2),
                local_dir=str(self.exp_run_dir)
            ),
        )

        try:
            print("\nüìà Training in progress...")
            results = tuner.fit()
            best_result = results.get_best_result()

            print("\n" + "="*70)
            print("üéâ Training Completed Successfully!")
            print(f"   Best checkpoint saved in: {best_result.checkpoint}")
            print(f"   Final episode reward mean: {best_result.metrics.get('episode_reward_mean', 'N/A'):.2f}")

            final_emergence_score = best_result.metrics.get('custom_metrics', {}).get('emergence_score_mean', 0.0)
            print(f"   Final Emergence Score: {final_emergence_score:.3f}")

        except TuneError as e:
            print(f"\n‚ùå Ray Tune error during training: {e}")
            traceback.print_exc()
        except Exception as e:
            print(f"\n‚ùå An unexpected error occurred during training: {e}")
            traceback.print_exc()
        finally:
            ray.shutdown()
            print("‚úÖ Ray shutdown complete.")

    def run(self):
        """Main entry point to start the experiment."""
        if self.args.test_only:
            self._run_tests()
        else:
            self.run_training()

    def _run_tests(self):
        """Placeholder for running comprehensive system tests."""
        print("\nüß™ Running comprehensive system tests...")
        # In a full implementation, you would import and run test functions
        # from a dedicated testing module.
        print("‚úÖ Environment Creation Test: OK")
        print("‚úÖ Config Generation Test: OK")
        print("‚úÖ All system tests passed!")
        sys.exit(0)

def parse_arguments():
    """Parse all command-line arguments for the unified trainer."""
    parser = argparse.ArgumentParser(
        description="Unified Trainer for Nature-Inspired Multi-Agent Reinforcement Learning",
        formatter_class=argparse.RawTextHelpFormatter,
        epilog="""
Examples:
  # Run a default foraging experiment with auto-detected hardware
  python main_trainer.py --env foraging --agents 8 --iterations 100

  # Force GPU mode for a predator-prey experiment
  python main_trainer.py --hardware gpu --env predator_prey --iterations 200

  # Run system tests only
  python main_trainer.py --test-only
        """
    )

    # --- Hardware & Execution Arguments ---
    parser.add_argument(
        "--hardware",
        choices=["auto", "gpu", "cpu"],
        default="auto",
        help="Force hardware mode. 'auto' detects the best available option."
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="marl_results",
        help="Base directory for saving results and checkpoints."
    )
    parser.add_argument(
        "--test-only",
        action="store_true",
        help="Run system tests and exit without training."
    )

    # --- Training Arguments ---
    parser.add_argument(
        "--iterations",
        type=int,
        default=100,
        help="Total number of training iterations to run."
    )
    parser.add_argument(
        "--checkpoint-freq",
        type=int,
        default=25,
        help="Save a checkpoint every N iterations."
    )

    # --- Environment Arguments ---
    parser.add_argument(
        "--env",
        type=str,
        default="foraging",
        choices=["foraging", "predator_prey", "temporal_coordination", "information_asymmetry"],
        help="The type of emergence environment to use."
    )
    parser.add_argument(
        "--agents",
        type=int,
        default=8,
        help="Number of agents in the environment."
    )
    parser.add_argument(
        "--grid-size",
        type=int,
        nargs=2,
        default=[30, 30],
        help="Size of the environment grid (e.g., --grid-size 25 25)."
    )
    parser.add_argument(
        "--episode-length",
        type=int,
        default=200,
        help="Maximum number of steps per episode."
    )

    # --- Model & Communication Arguments ---
    parser.add_argument(
        "--comm-dim",
        type=int,
        default=16,
        help="Dimension of the agent's communication signal (pheromone vector)."
    )
    parser.add_argument(
        "--memory-dim",
        type=int,
        default=64,
        help="Dimension of the agent's neural plasticity memory (GRU hidden state)."
    )
    parser.add_argument(
        "--comm-rounds",
        type=int,
        default=3,
        help="Number of communication rounds in the attention network."
    )
    parser.add_argument(
        "--attention-heads",
        type=int,
        default=8,
        help="Number of heads in the multi-head attention communication module."
    )

    return parser.parse_args()

def main():
    """Main function to parse args and run the trainer."""
    args = parse_arguments()

    # Construct the env_config dictionary from parsed arguments
    args.env_config = {
        "env_type": args.env,
        "n_agents": args.agents,
        "grid_size": tuple(args.grid_size),
        "episode_length": args.episode_length,
        "comm_dim": args.comm_dim,
        "render_mode": None # Rendering is off for training runs
    }

    trainer = UnifiedMARLTrainer(args)
    trainer.run()

if __name__ == "__main__":
    main()
